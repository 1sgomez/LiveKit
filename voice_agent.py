# voice_agent.py - Agente de voz 100% local con LiveKit
# Uso: python voice_agent.py --direct

import asyncio
import logging
import warnings
import os
import sys
import tempfile
import argparse
from typing import Optional
import numpy as np
import requests
import soundfile as sf
from pydub import AudioSegment
import aiohttp
from collections import deque
import librosa
import json  # ğŸ†• Para parsear data messages

# Importaciones de LiveKit
from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    WorkerOptions,
    cli,
)

# Importaciones de modelos
import whisper
from gtts import gTTS

from dotenv import load_dotenv

load_dotenv()

# === CONFIGURACIÃ“N ===
os.environ['LIVEKIT_LOG_LEVEL'] = 'error'
warnings.filterwarnings('ignore')

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

logging.getLogger('livekit.agents').setLevel(logging.ERROR)

# ConfiguraciÃ³n de LiveKit
LIVEKIT_URL = os.getenv('LIVEKIT_URL', 'ws://localhost:7880')
LIVEKIT_API_KEY = os.getenv('LIVEKIT_API_KEY')
LIVEKIT_API_SECRET = os.getenv('LIVEKIT_API_SECRET')
LIVEKIT_ROOM = os.getenv('LIVEKIT_ROOM', 'test')

class SimpleVAD:
    """Detector de actividad de voz simple basado en energÃ­a"""

    def __init__(self, threshold: float = 0.02, sample_rate: int = 16000):
        self.threshold = threshold
        self.sample_rate = sample_rate

    def is_speech(self, audio_data: np.ndarray) -> bool:
        """Detectar si hay voz basada en energÃ­a"""
        rms = np.sqrt(np.mean(audio_data ** 2))
        return rms > self.threshold

class LocalVoiceAgent:
    def __init__(self, room_name: str, use_simple_vad: bool = False):
        # Buffer de pre-roll (~300 ms)
        self.preroll_buffer = deque(maxlen=10)

        self.room_name = room_name
        self.ctx_room = None
        self.conversation_history = []
        self.audio_buffer = []
        self.is_processing = False
        self.silence_detected = False
        self.greeting_sent = False
        self.use_simple_vad = use_simple_vad

        # ğŸ†• SOLO agregamos esto para herramientas
        self.tool_data = {}  # Datos de herramientas recibidos

        # Cargar modelos
        logger.info("ğŸ“¥ Cargando Whisper (STT)...")
        self.whisper_model = whisper.load_model("small")

        # Intentar cargar Silero VAD
        self.vad = None
        self.simple_vad = SimpleVAD(threshold=0.004, sample_rate=16000)

        try:
            from livekit.plugins import silero
            logger.info("ğŸ“¥ Cargando Silero VAD...")
            self.vad = silero.VAD.load()
            logger.info("âœ… Silero VAD cargado")
        except Exception as e:
            logger.warning(f"âš ï¸ No se pudo cargar Silero VAD: {e}")
            logger.info("   Usando detector de volumen simple")

        logger.info("âœ… Modelos cargados")

    async def connect(self, url: str, token: str):
        """Conectar directamente a una sala"""
        logger.info(f"ğŸ”Œ Conectando a la sala {self.room_name}...")

        self.ctx_room = rtc.Room()

        self.ctx_room.on("track_subscribed", self.on_track_subscribed)
        self.ctx_room.on("participant_connected", self.on_participant_connected)
        self.ctx_room.on("connected", self.on_connected)
        self.ctx_room.on("data_received", self.on_data_received)  # ğŸ†• Agregar listener
        logger.info("âœ… Listeners registrados (incluido data_received)")  # ğŸ” Debug

        await self.ctx_room.connect(url, token)

        logger.info(f"âœ… Conectado a {self.room_name}")
        logger.info(f"ğŸ‘¤ Identity: {self.ctx_room.local_participant.identity}")

    def detect_speech(self, audio_data: np.ndarray) -> bool:
        """Detectar si hay voz"""
        if self.vad is not None:
            try:
                if hasattr(self.vad, 'is_speech'):
                    return self.vad.is_speech(audio_data, sample_rate=16000)
                elif callable(self.vad):
                    return self.vad(audio_data, sample_rate=16000) > 0.5
            except Exception as e:
                logger.debug(f"VAD error, usando fallback: {e}")

        return self.simple_vad.is_speech(audio_data)

    # ğŸ†• NUEVA FUNCIÃ“N: Recibir data messages
    def on_data_received(self, data_packet: rtc.DataPacket):
        """Callback: Recibir mensajes de datos del frontend"""
        logger.info("ğŸ“¨ Data packet recibido (RAW)")  # ğŸ” Debug
        try:
            payload_str = data_packet.data.decode('utf-8')
            logger.info(f"ğŸ“¨ Payload decodificado: {payload_str[:200]}")  # ğŸ” Debug
            payload = json.loads(payload_str)

            if payload.get('type') == 'tool_data':
                tool_name = payload.get('tool_name', 'unknown')
                tool_data = payload.get('data', {})

                logger.info(f"ğŸ› ï¸ Tool recibida: {tool_name}")
                logger.info(f"   Datos completos: {json.dumps(tool_data, indent=2, ensure_ascii=False)}")

                # Guardar datos
                self.tool_data[tool_name] = tool_data

                # ğŸ”¥ Agregar DATOS COMPLETOS al contexto
                context_msg = f"[Datos de {tool_name}]\n"
                context_msg += json.dumps(tool_data, indent=2, ensure_ascii=False)
                context_msg += "\n\nUsa esta informaciÃ³n para responder preguntas sobre estos campos/formularios."

                self.conversation_history.append({
                    "role": "system",
                    "content": context_msg
                })

                logger.info(f"âœ… Datos agregados al contexto (sin procesar)")

        except Exception as e:
            logger.error(f"âŒ Error procesando data message: {e}")

    async def transcribe_audio(self, audio_data: bytes, input_sample_rate: int) -> str:
        try:
            audio_np = (
                np.frombuffer(audio_data, dtype=np.int16)
                .astype(np.float32) / 32768.0
            )

            if input_sample_rate != 16000:
                audio_np = librosa.resample(
                    audio_np,
                    orig_sr=input_sample_rate,
                    target_sr=16000
                )

            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                wav_path = f.name
                sf.write(wav_path, audio_np, 16000, subtype="PCM_16")

            result = self.whisper_model.transcribe(
                wav_path,
                language="es",
                task="transcribe",
                initial_prompt="ConversaciÃ³n en espaÃ±ol. El usuario habla con un asistente llamado Jarvis.",
                temperature=0.0,
                fp16=False
            )

            os.unlink(wav_path)

            text = result["text"].strip()
            if text:
                logger.info(f"ğŸ‘¤ Usuario: {text}")

            return text

        except Exception as e:
            logger.error(f"âŒ Error en transcripciÃ³n: {e}")
            return ""

    async def generate_response(self, user_message: str) -> str:
        try:
            self.conversation_history.append(
                {"role": "user", "content": user_message}
            )

            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]

            # ğŸ†• Mejorar info de herramientas al system prompt
            tool_context = ""
            if self.tool_data:
                tool_context = "\n\n=== INFORMACIÃ“N IMPORTANTE ===\n"
                tool_context += "Tienes acceso a datos de campos/formularios:\n\n"

                for t_name, t_data in self.tool_data.items():
                    tool_context += f"â€¢ {t_name}:\n"
                    tool_context += json.dumps(t_data, indent=2, ensure_ascii=False)
                    tool_context += "\n\n"

                tool_context += "INSTRUCCIÃ“N: Cuando el usuario pregunte sobre campos, nombres o descripciones, "
                tool_context += "usa EXACTAMENTE la informaciÃ³n de arriba. No inventes ni supongas nada."

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "http://localhost:11434/api/chat",
                    json={
                        "model": "gemma3:1b",
                        "options": {
                            "temperature": 0.2,
                            "top_p": 0.9,
                            "num_predict": 60,
                            "repeat_penalty": 1.2
                        },
                        "messages": [
                            {
                                "role": "system",
                                "content":
                                "Eres Jarvis, un asistente de voz. "
                                "Respondes SOLO en espaÃ±ol. "
                                "Tus respuestas son cortas (mÃ¡x 2-3 frases). "
                                "Hablas de forma natural y directa. "
                                "No explicas limitaciones tÃ©cnicas. "
                                "Cuando tengas informaciÃ³n de campos/formularios en el contexto, "
                                "Ãºsala EXACTAMENTE como te la proporcionan. "
                                "Si te preguntan por un campo especÃ­fico, busca ese nombre en los datos "
                                "y responde con su descripciÃ³n. "
                                "\n\nğŸ–¥ï¸ COMANDOS VISUALES: "
                                "Si el usuario dice 'muÃ©strame en pantalla', 'ponlo en la pantalla', "
                                "'quiero verlo' o similar, responde normalmente pero agrega al FINAL: "
                                "[COMANDO:MOSTRAR]. Esto harÃ¡ que tu respuesta se muestre visualmente."
                                + tool_context
                            },
                            *self.conversation_history
                        ],
                        "stream": False
                    },
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as resp:

                    result = await resp.json()
                    assistant_message = result["message"]["content"].strip()

                    self.conversation_history.append(
                        {"role": "assistant", "content": assistant_message}
                    )

                    logger.info(f"ğŸ¤– Jarvis: {assistant_message}")
                    return assistant_message

        except Exception as e:
            logger.error(f"âŒ Error con Ollama: {e}")
            return "No puedo responder en este momento."

    async def synthesize_speech(self, text: str) -> Optional[bytes]:
        try:
            tts = gTTS(text=text, lang='es')

            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
                mp3_path = f.name
                tts.save(mp3_path)

            audio = AudioSegment.from_mp3(mp3_path).set_frame_rate(16000).set_channels(1)
            os.unlink(mp3_path)

            return audio.raw_data

        except Exception as e:
            logger.error(f"âŒ Error TTS: {e}")
            return None

    async def send_command_to_frontend(self, command: str, data: any = None):
        """
        ğŸ†• Enviar comandos al frontend vÃ­a data messages
        """
        try:
            if not self.ctx_room or not self.ctx_room.local_participant:
                logger.warning("âš ï¸ No hay sala para enviar comando")
                return

            payload = {
                'type': 'agent_command',
                'command': command,
                'data': data
            }

            logger.info(f"ğŸ“¤ Preparando comando: {json.dumps(payload, ensure_ascii=False)[:100]}")

            payload_bytes = json.dumps(payload).encode('utf-8')

            await self.ctx_room.local_participant.publish_data(
                payload_bytes,
                reliable=True
            )

            logger.info(f"âœ… Comando '{command}' enviado exitosamente")

        except Exception as e:
            logger.error(f"âŒ Error enviando comando: {e}")
            import traceback
            traceback.print_exc()

    async def publish_audio(self, audio_data: bytes):
        """Publicar audio en la sala"""
        try:
            sample_rate = 22050
            source = rtc.AudioSource(sample_rate, 1)
            track = rtc.LocalAudioTrack.create_audio_track("agent-voice", source)

            options = rtc.TrackPublishOptions()
            await self.ctx_room.local_participant.publish_track(track, options)
            logger.info("âœ… Audio publicado")

            audio_np = np.frombuffer(audio_data, dtype=np.int16)

            chunk_size = 2048
            for i in range(0, len(audio_np), chunk_size):
                chunk = audio_np[i:i+chunk_size]
                frame = rtc.AudioFrame(
                    data=chunk.tobytes(),
                    sample_rate=sample_rate,
                    num_channels=1,
                    samples_per_channel=len(chunk)
                )
                await source.capture_frame(frame)
                await asyncio.sleep(len(chunk) / sample_rate)

            logger.info("âœ… Audio reproducido")

        except Exception as e:
            logger.error(f"âŒ Error publicando: {e}")

    async def send_greeting(self):
        """Enviar saludo"""
        if self.greeting_sent:
            return

        greeting = "Â¡Hola! Soy Jarvis, tu asistente de voz. Â¿En quÃ© puedo ayudarte?"
        logger.info("ğŸ¤ Enviando saludo...")

        audio = await self.synthesize_speech(greeting)
        if audio:
            await self.publish_audio(audio)
            self.greeting_sent = True

    async def handle_audio_stream(self, track: rtc.Track, participant: rtc.RemoteParticipant):
        if self.is_processing:
            return

        self.is_processing = True
        self.audio_buffer = []
        self.preroll_buffer.clear()
        self.silence_detected = False
        silence_start = None
        speech_detected = False
        input_sample_rate = None

        logger.info(f"ğŸ¤ Escuchando a {participant.identity}...")

        audio_stream = rtc.AudioStream(track)
        silence_timeout = 1

        try:
            async for frame_event in audio_stream:
                current_time = asyncio.get_event_loop().time()

                input_sample_rate = frame_event.frame.sample_rate

                audio_data = (
                    np.frombuffer(frame_event.frame.data, dtype=np.int16)
                    .astype(np.float32) / 32768.0
                )

                self.preroll_buffer.append(frame_event.frame.data)

                is_speech = self.detect_speech(audio_data)

                if is_speech:
                    if not speech_detected:
                        self.audio_buffer.extend(self.preroll_buffer)
                        logger.debug("ğŸ™ï¸ Pre-roll agregado")

                    self.audio_buffer.append(frame_event.frame.data)
                    self.silence_detected = False
                    silence_start = None
                    speech_detected = True

                else:
                    if speech_detected:
                        if not self.silence_detected:
                            self.silence_detected = True
                            silence_start = current_time
                        elif silence_start and (current_time - silence_start) >= silence_timeout:
                            logger.info(
                                f"ğŸ›‘ Fin del habla ({current_time - silence_start:.2f}s silencio)"
                            )
                            break

            if self.audio_buffer:
                logger.info(f"ğŸ“ Procesando {len(self.audio_buffer)} frames de audio...")

                audio_bytes = b"".join(self.audio_buffer)

                text = await self.transcribe_audio(audio_bytes, input_sample_rate)

                if text:
                    response = await self.generate_response(text)

                    # AGREGAR ESTO:
                    logger.info(f"ğŸ” DEBUG - Respuesta: '{response}'")
                    logger.info(f"ğŸ” DEBUG - Upper: '{response.upper()}'")
                    logger.info(f"ğŸ” DEBUG - Contiene?: {('[COMANDO:MOSTRAR]' in response.upper())}")

                    should_display = '[COMANDO:MOSTRAR]' in response.upper()
                    clean_response = response.replace('[COMANDO:MOSTRAR]', '').replace('[comando:mostrar]', '').strip()

                    if should_display:
                        logger.info("ğŸ–¥ï¸ INTENTANDO ENVIAR COMANDO")
                        await self.send_command_to_frontend('mostrar_mensaje', clean_response)
                        logger.info("âœ… Comando enviado")
                    else:
                        logger.info("âŒ No se detectÃ³ comando")

                    audio_response = await self.synthesize_speech(response)
                    if audio_response:
                        await self.publish_audio(audio_response)
                else:
                    logger.info("ğŸ¤· Whisper no detectÃ³ texto")

        except Exception as e:
            logger.error(f"âŒ Error procesando audio: {e}")
            import traceback
            traceback.print_exc()

        finally:
            self.is_processing = False
            self.audio_buffer = []

    async def listen_loop(self, track: rtc.Track, participant: rtc.RemoteParticipant):
        while True:
            try:
                await self.handle_audio_stream(track, participant)
                await asyncio.sleep(0.3)
            except Exception as e:
                logger.error(f"âŒ Error en loop de escucha: {e}")
                await asyncio.sleep(1)

    def on_track_subscribed(self, track: rtc.Track, publication, participant: rtc.RemoteParticipant):
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"ğŸ§ Audio de {participant.identity}")
            asyncio.ensure_future(self.listen_loop(track, participant))

    def on_participant_connected(self, participant: rtc.RemoteParticipant):
        """Callback: participante conectado"""
        logger.info(f"ğŸ‘‹ {participant.identity} se uniÃ³")
        asyncio.ensure_future(self.send_greeting())

    def on_connected(self):
        """Callback: conectado"""
        logger.info("ğŸ”— ConexiÃ³n establecida")

        if self.ctx_room and len(self.ctx_room.remote_participants) > 0:
            asyncio.ensure_future(self.send_greeting())

    async def run(self):
        """Mantener agente activo"""
        logger.info("â³ Escuchando... (Ctrl+C para salir)")
        await asyncio.sleep(float('inf'))


async def check_dependencies():
    """Verificar dependencias"""
    logger.info("ğŸ” Verificando dependencias...")

    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            logger.info("âœ… Ollama disponible")
        else:
            logger.warning("âš ï¸ Ollama con error")
    except Exception as e:
        logger.error(f"âŒ Ollama no disponible: {e}")
        return False

    try:
        import whisper
        logger.info("âœ… Whisper disponible")
    except ImportError:
        logger.error("âŒ Whisper no instalado")
        return False

    return True


async def run_direct_mode():
    """Modo directo: conectar directamente a la sala"""
    logger.info("="*60)
    logger.info("ğŸš€ MODO DIRECTO - Conectando directamente a la sala")
    logger.info("="*60)

    if not await check_dependencies():
        logger.error("âŒ Dependencias no disponibles")
        return

    import time
    participant_name = f"agent-{int(time.time())}"

    logger.info(f"ğŸ“¡ Solicitando token para sala '{LIVEKIT_ROOM}'...")

    try:
        response = requests.get(
            f"http://localhost:5000/token",
            params={"room": LIVEKIT_ROOM, "name": participant_name},
            timeout=10
        )

        if response.status_code != 200:
            logger.error(f"âŒ Error obteniendo token: {response.status_code}")
            return

        data = response.json()
        token = data["token"]
        url = data["url"]

        logger.info(f"âœ… Token recibido")
        logger.info(f"ğŸ”— URL: {url}")

        agent = LocalVoiceAgent(LIVEKIT_ROOM)
        await agent.connect(url, token)
        await agent.run()

    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


async def entrypoint(ctx: JobContext):
    """Entry point modo worker"""
    logger.info("="*60)
    logger.info("ğŸš€ TRABAJO RECIBIDO - Modo worker")
    logger.info("="*60)

    if ctx is None:
        logger.error("âŒ Error: ctx es None")
        return

    try:
        await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

        if not ctx.room:
            logger.error("âŒ No hay sala")
            return

        if not await check_dependencies():
            return

        agent = LocalVoiceAgent(ctx.room.name)
        agent.ctx_room = ctx.room

        ctx.room.on("track_subscribed", agent.on_track_subscribed)
        ctx.room.on("participant_connected", agent.on_participant_connected)
        ctx.room.on("connected", agent.on_connected)
        ctx.room.on("data_received", agent.on_data_received)  # ğŸ†• Agregar

        logger.info(f"ğŸ“ Conectado a: {ctx.room.name}")
        await asyncio.sleep(float('inf'))

    except Exception as e:
        logger.error(f"âŒ Error: {e}")


def main():
    """FunciÃ³n principal"""
    parser = argparse.ArgumentParser(description="Agente de voz Jarvis")
    parser.add_argument("--direct", action="store_true",
                       help="Modo directo: conectar directamente a la sala")
    parser.add_argument("--simple-vad", action="store_true",
                       help="Usar detector de volumen simple en lugar de Silero VAD")
    args = parser.parse_args()

    print('='*60)
    print('ğŸš€ Voice Agent - Jarvis')
    print('='*60)

    print(f"\nğŸ“‹ CONFIGURACIÃ“N:")
    print(f"   LIVEKIT_URL: {LIVEKIT_URL}")
    print(f"   LIVEKIT_ROOM: {LIVEKIT_ROOM}")
    print()

    if not LIVEKIT_API_KEY or not LIVEKIT_API_SECRET:
        print("âŒ ERROR: Faltan LIVEKIT_API_KEY o LIVEKIT_API_SECRET")
        sys.exit(1)

    if args.direct:
        print("ğŸ¯ MODO: Directo")
        print("   El agente se conectarÃ¡ inmediatamente a la sala")
        print()

        try:
            asyncio.run(run_direct_mode())
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Agente detenido")
    else:
        print("ğŸ¯ MODO: Worker")
        print("   El agente esperarÃ¡ trabajos de LiveKit")
        print()

        try:
            cli.run_app(
                WorkerOptions(
                    entrypoint_fnc=entrypoint,
                    api_key=LIVEKIT_API_KEY,
                    api_secret=LIVEKIT_API_SECRET,
                    ws_url=LIVEKIT_URL,
                    worker_type="room",
                    num_idle_processes=1,
                )
            )
        except Exception as e:
            print(f'âŒ Error: {e}')
            print("\nğŸ’¡ Sugerencia: Usa el modo directo:")
            print("   python voice_agent.py --direct")
            sys.exit(1)


if __name__ == "__main__":
    main()
